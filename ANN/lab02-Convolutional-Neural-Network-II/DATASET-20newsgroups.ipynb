{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# DATASET [20newsgropus](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_20newsgroups.html#sklearn.datasets.fetch_20newsgroups)"
      ],
      "metadata": {
        "id": "1M5AZGO60hiL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-FAYJZGsr10",
        "outputId": "479f875e-5324-4d4c-900a-79f0718056e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "354/354 [==============================] - 83s 228ms/step - loss: 2.1958 - accuracy: 0.3840 - val_loss: 1.2347 - val_accuracy: 0.6555\n",
            "Epoch 2/5\n",
            "354/354 [==============================] - 63s 178ms/step - loss: 0.5769 - accuracy: 0.8533 - val_loss: 0.8154 - val_accuracy: 0.7627\n",
            "Epoch 3/5\n",
            "354/354 [==============================] - 57s 162ms/step - loss: 0.1732 - accuracy: 0.9651 - val_loss: 0.7339 - val_accuracy: 0.7856\n",
            "Epoch 4/5\n",
            "354/354 [==============================] - 51s 143ms/step - loss: 0.0465 - accuracy: 0.9951 - val_loss: 0.7285 - val_accuracy: 0.7942\n",
            "Epoch 5/5\n",
            "354/354 [==============================] - 43s 121ms/step - loss: 0.0172 - accuracy: 0.9987 - val_loss: 0.7410 - val_accuracy: 0.7970\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fa0263efd90>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from tensorflow.keras.layers import Input, Embedding, Conv1D, GlobalMaxPooling1D, Dense\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Wybor 4 kategorii\n",
        "# categories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']\n",
        "# newsgroups_train = fetch_20newsgroups(subset='train', categories=categories)\n",
        "# newsgroups_test = fetch_20newsgroups(subset='test', categories=categories)\n",
        "\n",
        "# Wybor wszystkich\n",
        "newsgroups_train = fetch_20newsgroups(subset='train')\n",
        "newsgroups_test = fetch_20newsgroups(subset='test')\n",
        "\n",
        "\n",
        "# Tokenizacja \n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=10000)\n",
        "tokenizer.fit_on_texts(newsgroups_train.data)\n",
        "x_train = tokenizer.texts_to_sequences(newsgroups_train.data)\n",
        "x_test = tokenizer.texts_to_sequences(newsgroups_test.data)\n",
        "\n",
        "# Ustalenie tej samej dlugosci dla wszystkich danych\n",
        "max_length = max(len(sequence) for sequence in x_train)\n",
        "x_train = pad_sequences(x_train, maxlen=max_length)\n",
        "x_test = pad_sequences(x_test, maxlen=max_length)\n",
        "\n",
        "# One-hot encode kategorii\n",
        "y_train = to_categorical(newsgroups_train.target)\n",
        "y_test = to_categorical(newsgroups_test.target)\n",
        "\n",
        "\n",
        "# Budowanie modelu\n",
        "inputs = Input(shape=(max_length,))\n",
        "x = Embedding(input_dim=10000, output_dim=100, input_length=max_length)(inputs)\n",
        "x = Conv1D(filters=64, kernel_size=5, activation='relu')(x)\n",
        "x = GlobalMaxPooling1D()(x)\n",
        "x = Dense(units=32, activation='relu')(x)\n",
        "outputs = Dense(units=20, activation='softmax')(x) #jak mniej kategorii to mniej neuronow na wyjsciu, zalezy od input.shape[1]\n",
        "\n",
        "model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "#Kompilacja i trenowanie\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train, epochs=5, batch_size=32, validation_data=(x_test, y_test))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Wykonaj predykcję dla 10 przypadków testowych\n",
        "predicted_probabilities = model.predict(x_test[:10])\n",
        "predicted_categories = np.argmax(predicted_probabilities, axis=1)\n",
        "\n",
        "# Wyświetl dane wejściowe i dane przewidywane przez model\n",
        "for i in range(len(predicted_categories)):\n",
        "    print(\"Input:\", newsgroups_test.data[i])\n",
        "    print(\"True category:\", newsgroups_train.target_names[newsgroups_test.target[i]])\n",
        "    print(\"Predicted category:\", newsgroups_train.target_names[predicted_categories[i]])\n",
        "    print(\"-----------------------------\")"
      ],
      "metadata": {
        "id": "Qe5ohUnbt2zO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X7swRFy_ztwb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}